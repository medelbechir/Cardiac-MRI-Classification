{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9979151e",
      "metadata": {},
      "source": [
        "# Final Pipeline - Mohamed MOHAMED EL BECHIR\n",
        "La pipeline finale pour l'extraction de features et l'entraînement de modèles (Random Forest, Gradient Boosting et XGBoost) dans le cadre du challenge Kaggle (***Cardiac Pathology Prediction***)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6543841d",
      "metadata": {},
      "source": [
        "## Structure des données\n",
        "\n",
        "Les données sont organisées dans un dossier `data/` contenant :\n",
        "\n",
        "- **`Train/`** : Données d'entraînement avec labels, sous forme de sous-dossiers par patient (`001/`, `002/`, etc.), chacun contenant :\n",
        "  - `XXX_ED.nii` et `XXX_ES.nii` : IRM en end-diastole (ED) et end-systole (ES).\n",
        "  - `XXX_ED_seg.nii` et `XXX_ES_seg.nii` : Segmentations associées.\n",
        "\n",
        "- **`Test/`** : Données de test, même structure que `Train/`, mais sans labels.\n",
        "\n",
        "### Reconstruction des données de test\n",
        "\n",
        "Le pipeline génère un dossier **`Test2/`** dans (`data/`), où les segmentations du ventricule gauche (VG) sont reconstruites selon la même organisation.\n",
        "\n",
        "### Métadonnées\n",
        "\n",
        "- `metadataTrain.csv` : Contient les IDs des patients et leurs labels.\n",
        "- `metadataTest.csv` : Contient uniquement les IDs des patients.\n",
        "\n",
        "### Catégories\n",
        "\n",
        "Les catégories (labels) représentent les diagnostics cardiaques suivants :\n",
        "- `0` : Healthy controls (Contrôles sains)\n",
        "- `1` : Myocardial infarction (Infarctus du myocarde)\n",
        "- `2` : Dilated cardiomyopathy (Cardiomyopathie dilatée)\n",
        "- `3` : Hypertrophic cardiomyopathy (Cardiomyopathie hypertrophique)\n",
        "- `4` : Abnormal right ventricle (Ventricule droit anormal)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "50c465c8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python : 3.11.12\n",
            "numpy version: 1.26.4\n",
            "pandas version: 2.2.3\n",
            "nibabel version: 5.3.2\n",
            "cv2 version: 4.11.0\n",
            "scipy version: 4.11.0\n",
            "sklearn version: 4.11.0\n",
            "xgboost version: 4.11.0\n",
            "pathlib version: 4.11.0\n"
          ]
        }
      ],
      "source": [
        "# ==== INFO VERSIONS ====\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "import cv2\n",
        "import os\n",
        "from scipy.stats import kurtosis, skew\n",
        "from pathlib import Path\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from xgboost import XGBClassifier\n",
        "from scipy import ndimage\n",
        "\n",
        "print(f\"Python : {sys.version.split()[0]}\")\n",
        "print(\"numpy version:\", np.__version__)\n",
        "print(\"pandas version:\", pd.__version__)\n",
        "print(\"nibabel version:\", nib.__version__)\n",
        "print(\"cv2 version:\", cv2.__version__)\n",
        "print(\"scipy version:\", cv2.__version__)\n",
        "print(\"sklearn version:\", cv2.__version__)\n",
        "print(\"xgboost version:\", cv2.__version__)\n",
        "print(\"pathlib version:\", cv2.__version__)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65494b4b",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:02<00:00, 18.29it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def segment_lv_from_myo(seg):\n",
        "    seg_lv = seg.copy()\n",
        "    # Remplir les trous slice par slice\n",
        "    for z in range(seg.shape[2]):\n",
        "        slice_ = seg[:,:,z]\n",
        "        myo = (slice_ == 2)\n",
        "        filled = ndimage.binary_fill_holes(myo)\n",
        "        lv = filled & (~myo)\n",
        "        seg_lv[:,:,z][lv] = 3\n",
        "    return seg_lv\n",
        "\n",
        "# Dossiers d'entrée et sortie\n",
        "train_dir = \"data/Test\"\n",
        "test_dir = \"data/Test2\"\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Liste des patients (dossiers)\n",
        "subject_ids = [d for d in sorted(os.listdir(train_dir)) if d.isdigit()]  # [\"101\", \"102\", ...]\n",
        "\n",
        "for subject_id in tqdm(subject_ids):\n",
        "    subject_train_path = os.path.join(train_dir, subject_id)\n",
        "    if not os.path.isdir(subject_train_path):\n",
        "        continue  # on saute les fichiers\n",
        "\n",
        "    # Dossier de sortie\n",
        "    subject_test_path = os.path.join(test_dir, subject_id)\n",
        "    os.makedirs(subject_test_path, exist_ok=True)\n",
        "\n",
        "    for phase in [\"ED\", \"ES\"]:\n",
        "        # Fichiers image et segmentation\n",
        "        img_path = os.path.join(subject_train_path, f\"{subject_id}_{phase}.nii\")\n",
        "        seg_path = os.path.join(subject_train_path, f\"{subject_id}_{phase}_seg.nii\")\n",
        "\n",
        "        if not os.path.exists(img_path) or not os.path.exists(seg_path):\n",
        "            print(f\"Fichiers manquants pour {subject_id} {phase}\")\n",
        "            continue\n",
        "\n",
        "        # Charger image (pas modifiée)\n",
        "        img_nii = nib.load(img_path)\n",
        "        img_data = img_nii.get_fdata()\n",
        "\n",
        "        # Charger segmentation et reconstruire le label 3\n",
        "        seg_nii = nib.load(seg_path)\n",
        "        seg_data = seg_nii.get_fdata().astype(np.uint8)  \n",
        "        seg_with_lv = segment_lv_from_myo(seg_data)\n",
        "        seg_with_lv = seg_with_lv.astype(np.uint8)      \n",
        "\n",
        "        # Sauvegarde image et segmentation reconstruite\n",
        "        nib.save(nib.Nifti1Image(img_data, img_nii.affine), os.path.join(subject_test_path, f\"{subject_id}_{phase}.nii\"))\n",
        "        nib.save(nib.Nifti1Image(seg_with_lv, seg_nii.affine), os.path.join(subject_test_path, f\"{subject_id}_{phase}_seg.nii\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e16a468a",
      "metadata": {},
      "source": [
        "## Fonctions de Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6ff234ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_binary_volume(mask: np.ndarray, spacing: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Calcule le volume binaire d'un masque 3D (nombre de voxels * volume de chaque voxel).\n",
        "    \"\"\"\n",
        "    return float(mask.sum() * np.prod(spacing))\n",
        "\n",
        "def compute_wall_thickness(mask3d: np.ndarray, spacing: np.ndarray):\n",
        "    \"\"\"\n",
        "    Calcule l'épaisseur moyenne, écart-type, max et min du 'myocarde' dans chaque slice 2D,\n",
        "    en utilisant la distance transform de OpenCV.\n",
        "    \"\"\"\n",
        "    scale = np.sqrt(spacing[0] + spacing[1])\n",
        "    thicknesses = [\n",
        "        cv2.distanceTransform(mask3d[:, :, z].astype(np.uint8), cv2.DIST_L2, 5).ptp() * scale\n",
        "        for z in range(mask3d.shape[2])\n",
        "    ]\n",
        "    arr = np.asarray(thicknesses)\n",
        "    return arr.mean(), arr.std(), arr.max(), arr.min()\n",
        "\n",
        "def compute_wall_circularity(mask3d: np.ndarray, spacing: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Calcule la circularité (4*pi*Surface / Périmètre^2) moyenne sur toutes les slices 2D.\n",
        "    \"\"\"\n",
        "    circ = []\n",
        "    scale = np.sqrt(spacing[0] + spacing[1])\n",
        "    for z in range(mask3d.shape[2]):\n",
        "        contours, _ = cv2.findContours(mask3d[:, :, z].astype(np.uint8),\n",
        "                                       cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        if contours:\n",
        "            area = cv2.contourArea(contours[0])\n",
        "            perimeter = cv2.arcLength(contours[0], True) * scale\n",
        "            if perimeter:\n",
        "                circ.append(4 * np.pi * area / perimeter**2)\n",
        "    return float(np.mean(circ)) if circ else 0.0\n",
        "\n",
        "def compute_wall_perimeter(mask3d: np.ndarray, spacing: np.ndarray):\n",
        "    \"\"\"\n",
        "    Retourne la moyenne, le max et le min des périmètres du myocarde sur toutes les slices 2D.\n",
        "    \"\"\"\n",
        "    perimeters = []\n",
        "    scale = np.sqrt(spacing[0] + spacing[1])\n",
        "    for z in range(mask3d.shape[2]):\n",
        "        contours, _ = cv2.findContours(mask3d[:, :, z].astype(np.uint8),\n",
        "                                       cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        perimeters.extend(cv2.arcLength(c, True) * scale for c in contours)\n",
        "    arr = np.asarray(perimeters)\n",
        "    if len(arr) == 0:\n",
        "        return 0.0, 0.0, 0.0\n",
        "    return arr.mean(), arr.max(), arr.min()\n",
        "\n",
        "def extract_patient_features(pid: int, root_dir: Path, *, label: int | None = None) -> dict:\n",
        "    \"\"\"\n",
        "    Extrait tous les features (anatomiques, géométriques, fonctionnels, etc.) pour un patient donné.\n",
        "    \"\"\"\n",
        "    sid = f\"{pid:03d}\"\n",
        "    seg_ed = nib.load(root_dir / sid / f\"{sid}_ED_seg.nii\")\n",
        "    seg_es = nib.load(root_dir / sid / f\"{sid}_ES_seg.nii\")\n",
        "\n",
        "    d_ed, d_es = seg_ed.get_fdata().astype(int), seg_es.get_fdata().astype(int)\n",
        "    v_ed, v_es = np.array(seg_ed.header.get_zooms()), np.array(seg_es.header.get_zooms())\n",
        "\n",
        "    record = {\"Id\": pid}\n",
        "    if label is not None:\n",
        "        record[\"Category\"] = label\n",
        "\n",
        "    # --- Anatomy features ---\n",
        "    anatomy_features = {\n",
        "        \"vol_rv_ED\": compute_binary_volume(d_ed == 1, v_ed),\n",
        "        \"vol_myo_ED\": compute_binary_volume(d_ed == 2, v_ed),\n",
        "        \"vol_lv_ED\": compute_binary_volume(d_ed == 3, v_ed),\n",
        "        \"vol_rv_ES\": compute_binary_volume(d_es == 1, v_es),\n",
        "        \"vol_myo_ES\": compute_binary_volume(d_es == 2, v_es),\n",
        "        \"vol_lv_ES\": compute_binary_volume(d_es == 3, v_es),\n",
        "    }\n",
        "    # Ratios\n",
        "    anatomy_features.update({\n",
        "        \"ratio_rv_lv_ED\": anatomy_features[\"vol_rv_ED\"] / anatomy_features[\"vol_lv_ED\"]\n",
        "                           if anatomy_features[\"vol_lv_ED\"] else 0.0,\n",
        "        \"ratio_myo_lv_ED\": anatomy_features[\"vol_myo_ED\"] / anatomy_features[\"vol_lv_ED\"]\n",
        "                            if anatomy_features[\"vol_lv_ED\"] else 0.0,\n",
        "        \"ratio_rv_lv_ES\": anatomy_features[\"vol_rv_ES\"] / anatomy_features[\"vol_lv_ES\"]\n",
        "                           if anatomy_features[\"vol_lv_ES\"] else 0.0,\n",
        "        \"ratio_myo_lv_ES\": anatomy_features[\"vol_myo_ES\"] / anatomy_features[\"vol_lv_ES\"]\n",
        "                            if anatomy_features[\"vol_lv_ES\"] else 0.0,\n",
        "    })\n",
        "\n",
        "    # --- Geometry features ---\n",
        "    th_mean_ed, th_std_ed, th_max_ed, th_min_ed = compute_wall_thickness(d_ed == 2, v_ed)\n",
        "    th_mean_es, th_std_es, th_max_es, th_min_es = compute_wall_thickness(d_es == 2, v_es)\n",
        "    peri_mean_ed, peri_max_ed, peri_min_ed = compute_wall_perimeter(d_ed == 2, v_ed)\n",
        "    peri_mean_es, peri_max_es, peri_min_es = compute_wall_perimeter(d_es == 2, v_es)\n",
        "\n",
        "    geometry_features = {\n",
        "        \"th_mean_ED\": th_mean_ed,\n",
        "        \"th_std_ED\": th_std_ed,\n",
        "        \"th_max_ED\": th_max_ed,\n",
        "        \"th_min_ED\": th_min_ed,\n",
        "        \"circ_ED\": compute_wall_circularity(d_ed == 2, v_ed),\n",
        "        \"peri_mean_ED\": peri_mean_ed,\n",
        "        \"peri_max_ED\": peri_max_ed,\n",
        "        \"peri_min_ED\": peri_min_ed,\n",
        "        \"th_mean_ES\": th_mean_es,\n",
        "        \"th_std_ES\": th_std_es,\n",
        "        \"th_max_ES\": th_max_es,\n",
        "        \"th_min_ES\": th_min_es,\n",
        "        \"circ_ES\": compute_wall_circularity(d_es == 2, v_es),\n",
        "        \"peri_mean_ES\": peri_mean_es,\n",
        "        \"peri_max_ES\": peri_max_es,\n",
        "        \"peri_min_ES\": peri_min_es,\n",
        "    }\n",
        "\n",
        "    # --- Functional features ---\n",
        "    function_features = {\n",
        "        \"EF_lv\": (anatomy_features[\"vol_lv_ED\"] - anatomy_features[\"vol_lv_ES\"])\n",
        "                  / anatomy_features[\"vol_lv_ED\"] if anatomy_features[\"vol_lv_ED\"] else 0.0,\n",
        "        \"EF_rv\": (anatomy_features[\"vol_rv_ED\"] - anatomy_features[\"vol_rv_ES\"])\n",
        "                  / anatomy_features[\"vol_rv_ED\"] if anatomy_features[\"vol_rv_ED\"] else 0.0,\n",
        "    }\n",
        "\n",
        "    # --- Diff features ---\n",
        "    diff_map = ((d_ed == 2).astype(int) - (d_es == 2).astype(int)).ravel() * np.prod(v_ed)\n",
        "    diff_features = {\n",
        "        \"diff_med\": float(np.median(diff_map)),\n",
        "        \"diff_std\": float(np.std(diff_map)),\n",
        "        \"diff_kurt\": float(kurtosis(diff_map)),\n",
        "        \"diff_skew\": float(skew(diff_map)),\n",
        "    }\n",
        "\n",
        "    # Assemblage final\n",
        "    record.update(anatomy_features)\n",
        "    record.update(geometry_features)\n",
        "    record.update(function_features)\n",
        "    record.update(diff_features)\n",
        "    return record\n",
        "\n",
        "def load_features(meta_csv: str, root: str, *, is_train: bool) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Charge toutes les features pour un ensemble de patients (train ou test)\n",
        "    en itérant sur un fichier CSV de métadonnées.\n",
        "    \"\"\"\n",
        "    meta = pd.read_csv(meta_csv)\n",
        "    root_dir = Path(root)\n",
        "    rows = [\n",
        "        extract_patient_features(int(row.Id), root_dir, label=int(row.Category) if is_train else None)\n",
        "        for _, row in meta.iterrows()\n",
        "    ]\n",
        "    return pd.DataFrame(rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21c57798",
      "metadata": {},
      "source": [
        "## Fonctions pour entraînement et combinaison de modèles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b38bca84",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_random_forest(X: pd.DataFrame, y: pd.Series) -> RandomForestClassifier:\n",
        "    \"\"\"\n",
        "    Entraîne un RandomForestClassifier avec des hyperparamètres fixés.\n",
        "    \"\"\"\n",
        "    rf = RandomForestClassifier(\n",
        "        bootstrap=True,\n",
        "        criterion='gini',\n",
        "        max_depth=40,\n",
        "        max_features='sqrt',\n",
        "        min_samples_leaf=2,\n",
        "        min_samples_split=8,\n",
        "        n_estimators=651,\n",
        "        random_state=0,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    rf.fit(X, y)\n",
        "    return rf\n",
        "\n",
        "def train_gradient_boosting(X: pd.DataFrame, y: pd.Series) -> GradientBoostingClassifier:\n",
        "    \"\"\"\n",
        "    Entraîne un GradientBoostingClassifier basique.\n",
        "    \"\"\"\n",
        "    gb = GradientBoostingClassifier(\n",
        "        n_estimators=100,\n",
        "        learning_rate=1,\n",
        "        max_depth=3,\n",
        "        random_state=34\n",
        "    )\n",
        "    gb.fit(X, y)\n",
        "    return gb\n",
        "\n",
        "def train_xgb_classifier(X: pd.DataFrame, y_bin: pd.Series) -> XGBClassifier:\n",
        "    \"\"\"\n",
        "    Entraîne un XGBClassifier sur des labels binaires (1 et 2).\n",
        "    \"\"\"\n",
        "    xgb = XGBClassifier(\n",
        "        objective=\"binary:logistic\",\n",
        "        eval_metric=\"logloss\",\n",
        "        n_estimators=100,\n",
        "        learning_rate=1,\n",
        "        max_depth=3,\n",
        "        random_state=34,\n",
        "        n_jobs=-1,\n",
        "    )\n",
        "    xgb.fit(X, y_bin)\n",
        "    return xgb\n",
        "\n",
        "def dynamic_override(pred_gb, proba_gb, gb_classes, pred_xgb, proba_xgb_bin, pred_xgb_bin):\n",
        "    \"\"\"\n",
        "    Remplace la prédiction du GB par celle du XGB si la confiance de XGB est plus élevée.\n",
        "    \"\"\"\n",
        "    idx_gb = {cls: i for i, cls in enumerate(gb_classes)}\n",
        "    final = pred_gb.copy()\n",
        "    for i, (pg, px) in enumerate(zip(pred_gb, pred_xgb)):\n",
        "        if proba_xgb_bin[i, pred_xgb_bin[i]] > proba_gb[i, idx_gb[pg]]:\n",
        "            final[i] = px\n",
        "    return final"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a88d9f9c",
      "metadata": {},
      "source": [
        "## Programme Principal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4ed3e56d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Liste de features sélectionnées pour la deuxième étape de classification \n",
        "# (tels que j'ai definis dans le notebook Feature_Model_Testing).\n",
        "FEAT2 = [\n",
        "    'vol_rv_ED', 'vol_myo_ED', 'vol_lv_ED',\n",
        "    'ratio_rv_lv_ED', 'ratio_myo_lv_ED',\n",
        "    'vol_rv_ES', 'vol_myo_ES', 'vol_lv_ES',\n",
        "    'th_mean_ES', 'ratio_rv_lv_ES',\n",
        "    'ratio_myo_lv_ES', 'EF_lv', 'EF_rv'\n",
        "]\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Exécute l'ensemble du pipeline :\n",
        "    1) Extraction des features\n",
        "    2) Entraînement d'un premier modèle (RandomForest)\n",
        "    3) Sélection des patients (cat. 1 ou 2) à affiner\n",
        "    4) Entraînement de modèles de niveau 2 (GB, XGB)\n",
        "    5) Fusion des prédictions\n",
        "    6) Sauvegarde du fichier submission.csv\n",
        "    \"\"\"\n",
        "    # 1. Feature extraction\n",
        "    df_train = load_features(\"metadataTrain.csv\", \"data/Train\", is_train=True)\n",
        "    df_test  = load_features(\"metadataTest.csv\",  \"data/Test2\",  is_train=False)\n",
        "\n",
        "    # Séparation features / labels pour l'entraînement\n",
        "    X, y = df_train.drop(columns=[\"Id\", \"Category\"]), df_train[\"Category\"].astype(int)\n",
        "    X_ts = df_test.drop(columns=[\"Id\"])\n",
        "\n",
        "    # 2. Random Forest de premier niveau\n",
        "    rf = train_random_forest(X, y)\n",
        "    pred_lvl1 = rf.predict(X_ts)\n",
        "\n",
        "    # 3. Focus sur les catégories {1, 2}\n",
        "    mask_tr, mask_ts = y.isin([1, 2]), np.isin(pred_lvl1, [1, 2])\n",
        "    X2_tr_gb  = X.loc[mask_tr, FEAT2]\n",
        "    X2_ts_gb  = X_ts.loc[mask_ts, FEAT2]\n",
        "    X2_tr_xgb = X.loc[mask_tr, FEAT2]\n",
        "    X2_ts_xgb = X_ts.loc[mask_ts, FEAT2]\n",
        "    y2_tr     = y[mask_tr]\n",
        "\n",
        "    # 4. Second niveau : entraînement et prédictions avec GB + XGB\n",
        "    gb2 = train_gradient_boosting(X2_tr_gb, y2_tr)\n",
        "    proba_gb = gb2.predict_proba(X2_ts_gb)\n",
        "    pred_gb  = gb2.predict(X2_ts_gb)\n",
        "\n",
        "    y2_tr_bin = y2_tr.map({1: 0, 2: 1})\n",
        "    xgb2 = train_xgb_classifier(X2_tr_xgb, y2_tr_bin)\n",
        "    proba_xgb_bin = xgb2.predict_proba(X2_ts_xgb)\n",
        "    pred_xgb_bin  = xgb2.predict(X2_ts_xgb)\n",
        "    pred_xgb      = np.where(pred_xgb_bin == 0, 1, 2)\n",
        "\n",
        "    # 5. Fusion finale des prédictions\n",
        "    pred_final_subset = dynamic_override(pred_gb, proba_gb, gb2.classes_,\n",
        "                                         pred_xgb, proba_xgb_bin, pred_xgb_bin)\n",
        "    final_pred = pred_lvl1.copy()\n",
        "    final_pred[mask_ts] = pred_final_subset\n",
        "\n",
        "    # 6. Export en CSV\n",
        "    pd.DataFrame({\n",
        "        \"Id\": df_test[\"Id\"].astype(int),\n",
        "        \"Category\": final_pred.astype(int)\n",
        "    }).to_csv(\"submission.csv\", index=False)\n",
        "    print(\"submission.csv saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ef1196bd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "submission.csv saved.\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
